{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "* Neural Machine Translation (NMT) model to translate human-readable dates (\"25th of June, 2009\") into machine-readable dates (\"2009-06-25\"). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Train the model on a dataset of 10,000 human readable dates and their equivalent, standardized, machine readable dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 23539.08it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dataset: a list of tuples of (human readable date, machine readable date).\n",
    "- human_vocab: a python dictionary mapping all characters used in the human readable dates to an integer-valued index.\n",
    "- machine_vocab: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. \n",
    "- inv_machine_vocab: the inverse dictionary of machine_vocab, mapping from indices back to characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('21 jan 2004', '2004-01-21'),\n",
       " ('01.07.20', '2020-07-01'),\n",
       " ('12/1/73', '1973-12-01'),\n",
       " ('thursday may 10 1973', '1973-05-10'),\n",
       " ('thursday january 13 2011', '2011-01-13'),\n",
       " ('thursday december 1 1994', '1994-12-01'),\n",
       " ('saturday december 21 2019', '2019-12-21'),\n",
       " ('29 feb 2016', '2016-02-29'),\n",
       " ('13 apr 2003', '2003-04-13'),\n",
       " ('thursday november 20 1980', '1980-11-20')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30 #maximum length on human written dates\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)#a processed version of the human readable dates in the training set\n",
    "print(\"Y.shape:\", Y.shape)#a processed version of the machine readable dates in the training set\n",
    "print(\"Xoh.shape:\", Xoh.shape)#one-hot version of X\n",
    "print(\"Yoh.shape:\", Yoh.shape)#one-hot version of Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: sunday september 16 2012\n",
      "Target date: 2012-09-16\n",
      "\n",
      "Source after preprocessing (indices): [29 31 25 16 13 34  0 29 17 27 30 17 24 14 17 28  0  4  9  0  5  3  4  5\n",
      " 36 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 3  1  2  3  0  1 10  0  2  7]\n",
      "\n",
      "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Target after preprocessing (one-hot): [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "index = 106\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "    <center>Entire model</center>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "    <center>Attention mechanism</center>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> Neural machine translation with attention</center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-attention and Post-attention LSTMs on both sides of the attention mechanism\n",
    "- There are two separate LSTMs in this model: pre-attention and post-attention LSTMs.\n",
    "- *Pre-attention* Bi-LSTM is the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism.\n",
    "    - The attention mechanism is shown in the middle of the left-hand diagram.\n",
    "    - The pre-attention Bi-LSTM goes through $T_x$ time steps\n",
    "- *Post-attention* LSTM: at the top of the diagram comes *after* the attention mechanism. \n",
    "    - The post-attention LSTM goes through $T_y$ time steps. \n",
    "\n",
    "- The post-attention LSTM passes the hidden state $s^{\\langle t \\rangle}$ and cell state $c^{\\langle t \\rangle}$ from one time step to the next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each time step does not use predictions from the previous time step\n",
    "* The post-attention LSTM at time $t$ does not take the previous time step's prediction $y^{\\langle t-1 \\rangle}$ as input.\n",
    "* The post-attention LSTM at time 't' only takes the hidden state $s^{\\langle t\\rangle}$ and cell state $c^{\\langle t\\rangle}$ as input. \n",
    "* The model is designed this way because there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The attention mechanism uses a `RepeatVector` node to copy $s^{\\langle t-1 \\rangle}$'s value $T_x$ times.\n",
    "- Then it uses `Concatenation` to concatenate $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$.\n",
    "- The concatenation of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ is fed into a \"Dense\" layer, which computes $e^{\\langle t, t' \\rangle}$. \n",
    "- $e^{\\langle t, t' \\rangle}$ is then passed through a softmax to compute $\\alpha^{\\langle t, t' \\rangle}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attention) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # repeator to repeat s_prev to be of shape (m, Tx, n_s) to concatenate it with all hidden states \"a\"\n",
    "    s_prev = repeator(s_prev)\n",
    "    # concatenator to concatenate a and s_prev on the last axis\n",
    "    concat = concatenator([a,s_prev])\n",
    "    # densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e.\n",
    "    e = densor1(concat)\n",
    "    #densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies.\n",
    "    energies = densor2(e)\n",
    "    # \"activator\" on \"energies\" to compute the attention weights \"alphas\" \n",
    "    alphas = activator(energies)\n",
    "    #dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell\n",
    "    context = dotor([alphas,a])\n",
    "    \n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 32 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
    "n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n",
    "\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # post-attention LSTM \n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # inputs of your model with a shape (Tx,)\n",
    "    # s0 (initial hidden state) and c0 (initial cell state) for the decoder LSTM with shape (n_s,)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pre-attention Bi-LSTM. \n",
    "    a = Bidirectional(LSTM(units=n_a, return_sequences=True))(X)\n",
    "    \n",
    "    # Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # one step of the attention mechanism to get back the context vector at step t \n",
    "        context = one_step_attention(a,s)\n",
    "        \n",
    "        # Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        s, _, c = post_activation_LSTM_cell(inputs=context, initial_state=[s,c])\n",
    "        \n",
    "        # Apply Dense layer to the hidden state output of the post-attention LSTM \n",
    "        out = output_layer(s)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Create model instance taking three inputs and returning the list of outputs.\n",
    "    model = Model(inputs = [X,s0,c0],outputs = outputs)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 30, 37)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 30, 64)       17920       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm_6[0][0]                     \n",
      "                                                                 lstm_6[1][0]                     \n",
      "                                                                 lstm_6[2][0]                     \n",
      "                                                                 lstm_6[3][0]                     \n",
      "                                                                 lstm_6[4][0]                     \n",
      "                                                                 lstm_6[5][0]                     \n",
      "                                                                 lstm_6[6][0]                     \n",
      "                                                                 lstm_6[7][0]                     \n",
      "                                                                 lstm_6[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 30, 128)      0           bidirectional_3[0][0]            \n",
      "                                                                 repeat_vector_2[0][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 repeat_vector_2[1][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 repeat_vector_2[2][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 repeat_vector_2[3][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 repeat_vector_2[4][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 repeat_vector_2[5][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 repeat_vector_2[6][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 repeat_vector_2[7][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 repeat_vector_2[8][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 repeat_vector_2[9][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 30, 10)       1290        concatenate_2[0][0]              \n",
      "                                                                 concatenate_2[1][0]              \n",
      "                                                                 concatenate_2[2][0]              \n",
      "                                                                 concatenate_2[3][0]              \n",
      "                                                                 concatenate_2[4][0]              \n",
      "                                                                 concatenate_2[5][0]              \n",
      "                                                                 concatenate_2[6][0]              \n",
      "                                                                 concatenate_2[7][0]              \n",
      "                                                                 concatenate_2[8][0]              \n",
      "                                                                 concatenate_2[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 30, 1)        11          dense_7[0][0]                    \n",
      "                                                                 dense_7[1][0]                    \n",
      "                                                                 dense_7[2][0]                    \n",
      "                                                                 dense_7[3][0]                    \n",
      "                                                                 dense_7[4][0]                    \n",
      "                                                                 dense_7[5][0]                    \n",
      "                                                                 dense_7[6][0]                    \n",
      "                                                                 dense_7[7][0]                    \n",
      "                                                                 dense_7[8][0]                    \n",
      "                                                                 dense_7[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_8[0][0]                    \n",
      "                                                                 dense_8[1][0]                    \n",
      "                                                                 dense_8[2][0]                    \n",
      "                                                                 dense_8[3][0]                    \n",
      "                                                                 dense_8[4][0]                    \n",
      "                                                                 dense_8[5][0]                    \n",
      "                                                                 dense_8[6][0]                    \n",
      "                                                                 dense_8[7][0]                    \n",
      "                                                                 dense_8[8][0]                    \n",
      "                                                                 dense_8[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 64), (None,  33024       dot_2[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_2[1][0]                      \n",
      "                                                                 lstm_6[0][0]                     \n",
      "                                                                 lstm_6[0][2]                     \n",
      "                                                                 dot_2[2][0]                      \n",
      "                                                                 lstm_6[1][0]                     \n",
      "                                                                 lstm_6[1][2]                     \n",
      "                                                                 dot_2[3][0]                      \n",
      "                                                                 lstm_6[2][0]                     \n",
      "                                                                 lstm_6[2][2]                     \n",
      "                                                                 dot_2[4][0]                      \n",
      "                                                                 lstm_6[3][0]                     \n",
      "                                                                 lstm_6[3][2]                     \n",
      "                                                                 dot_2[5][0]                      \n",
      "                                                                 lstm_6[4][0]                     \n",
      "                                                                 lstm_6[4][2]                     \n",
      "                                                                 dot_2[6][0]                      \n",
      "                                                                 lstm_6[5][0]                     \n",
      "                                                                 lstm_6[5][2]                     \n",
      "                                                                 dot_2[7][0]                      \n",
      "                                                                 lstm_6[6][0]                     \n",
      "                                                                 lstm_6[6][2]                     \n",
      "                                                                 dot_2[8][0]                      \n",
      "                                                                 lstm_6[7][0]                     \n",
      "                                                                 lstm_6[7][2]                     \n",
      "                                                                 dot_2[9][0]                      \n",
      "                                                                 lstm_6[8][0]                     \n",
      "                                                                 lstm_6[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 11)           715         lstm_6[0][0]                     \n",
      "                                                                 lstm_6[1][0]                     \n",
      "                                                                 lstm_6[2][0]                     \n",
      "                                                                 lstm_6[3][0]                     \n",
      "                                                                 lstm_6[4][0]                     \n",
      "                                                                 lstm_6[5][0]                     \n",
      "                                                                 lstm_6[6][0]                     \n",
      "                                                                 lstm_6[7][0]                     \n",
      "                                                                 lstm_6[8][0]                     \n",
      "                                                                 lstm_6[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 0.005,beta_1 = 0.9,beta_2 = 0.999,decay = 0.01)\n",
    "model.compile(optimizer = opt, loss = \"categorical_crossentropy\",metrics = [\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 17s 166ms/step - loss: 16.2540 - dense_9_loss: 1.2763 - dense_9_1_loss: 0.9822 - dense_9_2_loss: 1.7151 - dense_9_3_loss: 2.6647 - dense_9_4_loss: 0.7783 - dense_9_5_loss: 1.2300 - dense_9_6_loss: 2.6049 - dense_9_7_loss: 0.8353 - dense_9_8_loss: 1.6583 - dense_9_9_loss: 2.5089 - dense_9_acc: 0.4885 - dense_9_1_acc: 0.7012 - dense_9_2_acc: 0.3302 - dense_9_3_acc: 0.0941 - dense_9_4_acc: 0.9299 - dense_9_5_acc: 0.3851 - dense_9_6_acc: 0.0938 - dense_9_7_acc: 0.9072 - dense_9_8_acc: 0.2950 - dense_9_9_acc: 0.1229\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 20s 197ms/step - loss: 8.3643 - dense_9_loss: 0.2006 - dense_9_1_loss: 0.1338 - dense_9_2_loss: 0.8864 - dense_9_3_loss: 2.0805 - dense_9_4_loss: 0.0233 - dense_9_5_loss: 0.3875 - dense_9_6_loss: 1.7435 - dense_9_7_loss: 0.0165 - dense_9_8_loss: 0.9652 - dense_9_9_loss: 1.9269 - dense_9_acc: 0.9549 - dense_9_1_acc: 0.9630 - dense_9_2_acc: 0.6622 - dense_9_3_acc: 0.2427 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.8547 - dense_9_6_acc: 0.3907 - dense_9_7_acc: 0.9998 - dense_9_8_acc: 0.6218 - dense_9_9_acc: 0.2909\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 20s 196ms/step - loss: 6.4789 - dense_9_loss: 0.1159 - dense_9_1_loss: 0.0899 - dense_9_2_loss: 0.5995 - dense_9_3_loss: 1.7112 - dense_9_4_loss: 0.0133 - dense_9_5_loss: 0.2397 - dense_9_6_loss: 1.4632 - dense_9_7_loss: 0.0120 - dense_9_8_loss: 0.7886 - dense_9_9_loss: 1.4456 - dense_9_acc: 0.9695 - dense_9_1_acc: 0.9735 - dense_9_2_acc: 0.7914 - dense_9_3_acc: 0.3865 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9249 - dense_9_6_acc: 0.4820 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.6928 - dense_9_9_acc: 0.4867\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 4.8553 - dense_9_loss: 0.0883 - dense_9_1_loss: 0.0696 - dense_9_2_loss: 0.4485 - dense_9_3_loss: 1.1913 - dense_9_4_loss: 0.0116 - dense_9_5_loss: 0.1794 - dense_9_6_loss: 1.1986 - dense_9_7_loss: 0.0136 - dense_9_8_loss: 0.6757 - dense_9_9_loss: 0.9787 - dense_9_acc: 0.9753 - dense_9_1_acc: 0.9786 - dense_9_2_acc: 0.8455 - dense_9_3_acc: 0.6123 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9524 - dense_9_6_acc: 0.5702 - dense_9_7_acc: 0.9999 - dense_9_8_acc: 0.7400 - dense_9_9_acc: 0.6686\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 3.7389 - dense_9_loss: 0.0721 - dense_9_1_loss: 0.0597 - dense_9_2_loss: 0.3665 - dense_9_3_loss: 0.8008 - dense_9_4_loss: 0.0102 - dense_9_5_loss: 0.1389 - dense_9_6_loss: 0.9717 - dense_9_7_loss: 0.0146 - dense_9_8_loss: 0.5970 - dense_9_9_loss: 0.7075 - dense_9_acc: 0.9784 - dense_9_1_acc: 0.9814 - dense_9_2_acc: 0.8700 - dense_9_3_acc: 0.7819 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9676 - dense_9_6_acc: 0.6627 - dense_9_7_acc: 0.9997 - dense_9_8_acc: 0.7805 - dense_9_9_acc: 0.7654\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 17s 167ms/step - loss: 2.9745 - dense_9_loss: 0.0605 - dense_9_1_loss: 0.0478 - dense_9_2_loss: 0.3034 - dense_9_3_loss: 0.5478 - dense_9_4_loss: 0.0091 - dense_9_5_loss: 0.1176 - dense_9_6_loss: 0.8000 - dense_9_7_loss: 0.0122 - dense_9_8_loss: 0.5314 - dense_9_9_loss: 0.5446 - dense_9_acc: 0.9813 - dense_9_1_acc: 0.9845 - dense_9_2_acc: 0.8852 - dense_9_3_acc: 0.8556 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9723 - dense_9_6_acc: 0.7426 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.8089 - dense_9_9_acc: 0.8267\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 16s 165ms/step - loss: 2.4249 - dense_9_loss: 0.0513 - dense_9_1_loss: 0.0376 - dense_9_2_loss: 0.2518 - dense_9_3_loss: 0.4081 - dense_9_4_loss: 0.0069 - dense_9_5_loss: 0.1023 - dense_9_6_loss: 0.6667 - dense_9_7_loss: 0.0103 - dense_9_8_loss: 0.4730 - dense_9_9_loss: 0.4169 - dense_9_acc: 0.9842 - dense_9_1_acc: 0.9878 - dense_9_2_acc: 0.9055 - dense_9_3_acc: 0.8822 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9742 - dense_9_6_acc: 0.8038 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.8292 - dense_9_9_acc: 0.8743\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 2.0088 - dense_9_loss: 0.0427 - dense_9_1_loss: 0.0280 - dense_9_2_loss: 0.2016 - dense_9_3_loss: 0.3305 - dense_9_4_loss: 0.0053 - dense_9_5_loss: 0.0903 - dense_9_6_loss: 0.5497 - dense_9_7_loss: 0.0086 - dense_9_8_loss: 0.4239 - dense_9_9_loss: 0.3283 - dense_9_acc: 0.9865 - dense_9_1_acc: 0.9923 - dense_9_2_acc: 0.9281 - dense_9_3_acc: 0.8965 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9765 - dense_9_6_acc: 0.8496 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.8466 - dense_9_9_acc: 0.9035\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 17s 170ms/step - loss: 1.6856 - dense_9_loss: 0.0355 - dense_9_1_loss: 0.0217 - dense_9_2_loss: 0.1635 - dense_9_3_loss: 0.2781 - dense_9_4_loss: 0.0041 - dense_9_5_loss: 0.0790 - dense_9_6_loss: 0.4506 - dense_9_7_loss: 0.0073 - dense_9_8_loss: 0.3797 - dense_9_9_loss: 0.2662 - dense_9_acc: 0.9885 - dense_9_1_acc: 0.9944 - dense_9_2_acc: 0.9476 - dense_9_3_acc: 0.9110 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9798 - dense_9_6_acc: 0.8798 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.8621 - dense_9_9_acc: 0.9235\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 17s 166ms/step - loss: 1.4208 - dense_9_loss: 0.0304 - dense_9_1_loss: 0.0174 - dense_9_2_loss: 0.1279 - dense_9_3_loss: 0.2347 - dense_9_4_loss: 0.0032 - dense_9_5_loss: 0.0707 - dense_9_6_loss: 0.3744 - dense_9_7_loss: 0.0063 - dense_9_8_loss: 0.3365 - dense_9_9_loss: 0.2192 - dense_9_acc: 0.9902 - dense_9_1_acc: 0.9968 - dense_9_2_acc: 0.9665 - dense_9_3_acc: 0.9296 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9819 - dense_9_6_acc: 0.9013 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.8827 - dense_9_9_acc: 0.9390\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 17s 170ms/step - loss: 1.2096 - dense_9_loss: 0.0262 - dense_9_1_loss: 0.0142 - dense_9_2_loss: 0.0996 - dense_9_3_loss: 0.1948 - dense_9_4_loss: 0.0027 - dense_9_5_loss: 0.0638 - dense_9_6_loss: 0.3178 - dense_9_7_loss: 0.0056 - dense_9_8_loss: 0.2983 - dense_9_9_loss: 0.1866 - dense_9_acc: 0.9922 - dense_9_1_acc: 0.9981 - dense_9_2_acc: 0.9793 - dense_9_3_acc: 0.9496 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9833 - dense_9_6_acc: 0.9182 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.9007 - dense_9_9_acc: 0.9456\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 17s 165ms/step - loss: 1.0403 - dense_9_loss: 0.0231 - dense_9_1_loss: 0.0117 - dense_9_2_loss: 0.0759 - dense_9_3_loss: 0.1615 - dense_9_4_loss: 0.0023 - dense_9_5_loss: 0.0581 - dense_9_6_loss: 0.2745 - dense_9_7_loss: 0.0050 - dense_9_8_loss: 0.2654 - dense_9_9_loss: 0.1626 - dense_9_acc: 0.9934 - dense_9_1_acc: 0.9990 - dense_9_2_acc: 0.9881 - dense_9_3_acc: 0.9704 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9842 - dense_9_6_acc: 0.9294 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.9159 - dense_9_9_acc: 0.9536\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 0.8957 - dense_9_loss: 0.0203 - dense_9_1_loss: 0.0100 - dense_9_2_loss: 0.0597 - dense_9_3_loss: 0.1334 - dense_9_4_loss: 0.0020 - dense_9_5_loss: 0.0532 - dense_9_6_loss: 0.2354 - dense_9_7_loss: 0.0043 - dense_9_8_loss: 0.2343 - dense_9_9_loss: 0.1431 - dense_9_acc: 0.9949 - dense_9_1_acc: 0.9996 - dense_9_2_acc: 0.9926 - dense_9_3_acc: 0.9831 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9851 - dense_9_6_acc: 0.9417 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.9284 - dense_9_9_acc: 0.9607\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 0.7810 - dense_9_loss: 0.0182 - dense_9_1_loss: 0.0086 - dense_9_2_loss: 0.0483 - dense_9_3_loss: 0.1118 - dense_9_4_loss: 0.0018 - dense_9_5_loss: 0.0484 - dense_9_6_loss: 0.2059 - dense_9_7_loss: 0.0040 - dense_9_8_loss: 0.2090 - dense_9_9_loss: 0.1252 - dense_9_acc: 0.9965 - dense_9_1_acc: 0.9998 - dense_9_2_acc: 0.9949 - dense_9_3_acc: 0.9920 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9865 - dense_9_6_acc: 0.9491 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.9387 - dense_9_9_acc: 0.9674\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 17s 169ms/step - loss: 0.6895 - dense_9_loss: 0.0165 - dense_9_1_loss: 0.0073 - dense_9_2_loss: 0.0394 - dense_9_3_loss: 0.0963 - dense_9_4_loss: 0.0015 - dense_9_5_loss: 0.0444 - dense_9_6_loss: 0.1805 - dense_9_7_loss: 0.0036 - dense_9_8_loss: 0.1887 - dense_9_9_loss: 0.1113 - dense_9_acc: 0.9970 - dense_9_1_acc: 0.9999 - dense_9_2_acc: 0.9963 - dense_9_3_acc: 0.9943 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9875 - dense_9_6_acc: 0.9590 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.9445 - dense_9_9_acc: 0.9720\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 15s 152ms/step - loss: 0.6074 - dense_9_loss: 0.0149 - dense_9_1_loss: 0.0066 - dense_9_2_loss: 0.0336 - dense_9_3_loss: 0.0830 - dense_9_4_loss: 0.0014 - dense_9_5_loss: 0.0411 - dense_9_6_loss: 0.1568 - dense_9_7_loss: 0.0034 - dense_9_8_loss: 0.1678 - dense_9_9_loss: 0.0987 - dense_9_acc: 0.9977 - dense_9_1_acc: 0.9999 - dense_9_2_acc: 0.9972 - dense_9_3_acc: 0.9955 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9879 - dense_9_6_acc: 0.9669 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.9523 - dense_9_9_acc: 0.9758\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 16s 164ms/step - loss: 0.5376 - dense_9_loss: 0.0134 - dense_9_1_loss: 0.0059 - dense_9_2_loss: 0.0285 - dense_9_3_loss: 0.0732 - dense_9_4_loss: 0.0013 - dense_9_5_loss: 0.0379 - dense_9_6_loss: 0.1384 - dense_9_7_loss: 0.0030 - dense_9_8_loss: 0.1502 - dense_9_9_loss: 0.0858 - dense_9_acc: 0.9986 - dense_9_1_acc: 0.9999 - dense_9_2_acc: 0.9981 - dense_9_3_acc: 0.9962 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9887 - dense_9_6_acc: 0.9736 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.9597 - dense_9_9_acc: 0.9814\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.4815 - dense_9_loss: 0.0124 - dense_9_1_loss: 0.0053 - dense_9_2_loss: 0.0246 - dense_9_3_loss: 0.0658 - dense_9_4_loss: 0.0012 - dense_9_5_loss: 0.0354 - dense_9_6_loss: 0.1252 - dense_9_7_loss: 0.0028 - dense_9_8_loss: 0.1334 - dense_9_9_loss: 0.0754 - dense_9_acc: 0.9988 - dense_9_1_acc: 1.0000 - dense_9_2_acc: 0.9984 - dense_9_3_acc: 0.9968 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9896 - dense_9_6_acc: 0.9759 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.9639 - dense_9_9_acc: 0.9844\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 0.4292 - dense_9_loss: 0.0112 - dense_9_1_loss: 0.0049 - dense_9_2_loss: 0.0215 - dense_9_3_loss: 0.0591 - dense_9_4_loss: 0.0011 - dense_9_5_loss: 0.0327 - dense_9_6_loss: 0.1106 - dense_9_7_loss: 0.0026 - dense_9_8_loss: 0.1185 - dense_9_9_loss: 0.0670 - dense_9_acc: 0.9993 - dense_9_1_acc: 1.0000 - dense_9_2_acc: 0.9988 - dense_9_3_acc: 0.9971 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9894 - dense_9_6_acc: 0.9817 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.9710 - dense_9_9_acc: 0.9867\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 17s 165ms/step - loss: 0.3856 - dense_9_loss: 0.0103 - dense_9_1_loss: 0.0044 - dense_9_2_loss: 0.0190 - dense_9_3_loss: 0.0545 - dense_9_4_loss: 0.0010 - dense_9_5_loss: 0.0305 - dense_9_6_loss: 0.1000 - dense_9_7_loss: 0.0024 - dense_9_8_loss: 0.1055 - dense_9_9_loss: 0.0578 - dense_9_acc: 0.9994 - dense_9_1_acc: 1.0000 - dense_9_2_acc: 0.9990 - dense_9_3_acc: 0.9971 - dense_9_4_acc: 1.0000 - dense_9_5_acc: 0.9905 - dense_9_6_acc: 0.9853 - dense_9_7_acc: 1.0000 - dense_9_8_acc: 0.9759 - dense_9_9_acc: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9edaa308d0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dense_2_acc_8: 0.8562` means that you are predicting the 7th character of the output correctly 85.62% of the time in the current batch of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 3 May 1979\n",
      "source: [ 2 10  8 10  0  1  6  0  1  4]\n",
      "output: 1979-05-03\n",
      "input: 5 April 09\n",
      "source: [ 3  1 10 10  0  1  5  0  1  6]\n",
      "output: 2099-04-05\n",
      "input: 21th of August 2016\n",
      "source: [3 1 2 7 0 1 9 0 1 1]\n",
      "output: 2016-08-00\n",
      "input: Tue 10 Jul 2007\n",
      "source: [3 1 1 8 0 1 8 0 2 1]\n",
      "output: 2007-07-10\n",
      "input: Saturday May 9 2018\n",
      "source: [ 3  1  2  9  0  1  6  0  1 10]\n",
      "output: 2018-05-09\n",
      "input: March 3 2001\n",
      "source: [3 1 1 2 0 1 4 0 1 4]\n",
      "output: 2001-03-03\n",
      "input: March 3rd 2001\n",
      "source: [3 1 1 2 0 1 4 0 1 4]\n",
      "output: 2001-03-03\n",
      "input: 1 March 2001\n",
      "source: [3 1 1 2 0 1 4 0 1 2]\n",
      "output: 2001-03-01\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "\n",
    "source = np.array([string_to_int(i, Tx, human_vocab) for i in EXAMPLES])\n",
    "\n",
    "source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "\n",
    "s0 = np.zeros((len(EXAMPLES), n_s)) #LSTM cells must have the same number of rows as there are training examples\n",
    "c0 = np.zeros((len(EXAMPLES), n_s))\n",
    "\n",
    "prediction = model.predict([source, s0, c0])\n",
    "\n",
    "prediction = np.argmax(prediction, axis = -1).swapaxes(1,0)\n",
    "\n",
    "for t in range(prediction.shape[0]):\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction[t]]\n",
    "\n",
    "    print(\"input:\", EXAMPLES[t])\n",
    "    print(\"source:\", prediction[t])\n",
    "    print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "from tkinter import ttk\n",
    "\n",
    "win = tk.Tk()\n",
    "\n",
    "win.title('Machine Translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateMessage():\n",
    "    EXAMPLES = [humanDate.get()]\n",
    "\n",
    "    source = np.array([string_to_int(i, Tx, human_vocab) for i in EXAMPLES])\n",
    "\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "\n",
    "    s0 = np.zeros((len(EXAMPLES), n_s)) #LSTM cells must have the same number of rows as there are training examples\n",
    "    c0 = np.zeros((len(EXAMPLES), n_s))\n",
    "\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "\n",
    "    prediction = np.argmax(prediction, axis = -1).swapaxes(1,0)\n",
    "\n",
    "    for t in range(prediction.shape[0]):\n",
    "        output = [inv_machine_vocab[int(i)] for i in prediction[t]]\n",
    "        tk.Label(win, text = ''.join(output)).grid(row = 1, column = 1)\n",
    "        print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: 2001-08-13\n"
     ]
    }
   ],
   "source": [
    "tk.Label(win, text=\"Your Date\").grid(row=0)\n",
    "tk.Label(win, text=\"Machine Translation\").grid(row=1)\n",
    "\n",
    "humanDate = tk.Entry(win)\n",
    "\n",
    "\n",
    "humanDate.grid(row=0, column=1)\n",
    "\n",
    "\n",
    "tk.Button(win,text='Translate', command=translateMessage).grid(row=3)\n",
    "\n",
    "\n",
    "    \n",
    "win.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
